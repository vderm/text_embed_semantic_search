---
author:
- Vasken Dermardiros
categories: website
draft: false
lastmod: 2020-06-19 12:03:13-04:00
tags:
- research
- tensorflow
- quantization
title: Quantization
links:
- https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html
---

<https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html>

Quantization Aware Training with TensorFlow Model Optimization Toolkit -
Performance with Accuracy

The core idea is that QAT simulates low-precision inference-time computation in
the forward pass of the training process. This [work](https://arxiv.org/pdf/1712.05877.pdf) is credited to the original
innovations by Skirmantas Kligys in the Google Mobile Vision team. This
introduces the quantization error as noise during the training and as part of
the overall loss, which the optimization algorithm tries to minimize. Hence, the
model learns parameters that are more robust to quantization.

### Emulating low-precision computation {#emulating-low-precision-computation}

The training graph itself operates in floating-point (e.g. float32), but it has
to emulate low-precision computation, which is fixed-point (e.g. int8 in the
case of TensorFlow Lite). To do so, we insert special operations into the graph
(tensorflow::ops::FakeQuantWithMinMaxVars) that convert the floating-point
tensors into low-precision values and then convert the low-precision values back
into floating-point. This ensures that losses from quantization are introduced
in the computation and that further computations emulate low-precision. In order
to do so, we ensure that the losses from quantization are introduced in the
tensor and, since each value in the floating-point tensor now maps 1:1 to a
low-precision value, any further computation with similarly mapped tensors wonâ€™t
introduce any further loss and mimics low-precision computations exactly.

### Placing the quantization emulation operations {#placing-the-quantization-emulation-operations}

The quantization emulation operations need to be placed in the training graph
such that they are consistent with the way that the quantized graph will be
computed. This means that, for our API to be able to execute in TensorFlow Lite,
we needed to follow the TensorFlow Lite quantization spec precisely.

![[screenshot-04.png]]

### Quantize the entire Keras model {#quantize-the-entire-keras-model}

```python
import tensorflow_model_optimization as tfmot

model = tf.keras.Sequential([
   ...
])
# Quantize the entire model.
quantized_model = tfmot.quantization.keras.quantize_model(model)

# Continue with training as usual.
quantized_model.compile(...)
quantized_model.fit(...)
```

### Quantize part(s) of a Keras model {#quantize-part--s--of-a-keras-model}

```python
import tensorflow_model_optimization as tfmot
quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer

model = tf.keras.Sequential([
   ...
   # Only annotated layers will be quantized.
   quantize_annotate_layer(Conv2D()),
   quantize_annotate_layer(ReLU()),
   Dense(),
   ...
])

# Quantize the model.
quantized_model = tfmot.quantization.keras.quantize_apply(mode
```

## Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference {#quantization-and-training-of-neural-networks-for-efficient-integer-arithmetic-only-inference}

- <sup id="001c92a7a11815b89568bfe325cb399a"><a href="#jacob_quantization_2017" title="Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam \&amp; Kalenichenko, Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}, {arXiv:1712.05877 [cs, stat]}, v(), (2017).">jacob_quantization_2017</a></sup>

[//begin]: # "Autogenerated link references for markdown compatibility"
[[screenshot-04.png]: ../attachments/images/screenshot-04.png "screenshot-04.png"
[//end]: # "Autogenerated link references"