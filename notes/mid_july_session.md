---
author:
- Federico
- Fahimeh
- Fatma M
categories: note
draft: false
lastmod: 2021-01-03 22:49:12-05:00
slug: mid_july_session
tags:
- reading-week
- research
title: Mid-July Session
---


## Model Assertions for Monitoring and Improving ML Models {#model-assertions-for-monitoring-and-improving-ml-models}

<sup id="cbb978bc3ce70366de0d0bbfa67a708a"><a href="#Kang_ModelAssertionsMonitoring_2020" title="Kang, Raghavan, Bailis \&amp; Zaharia, Model {{Assertions}} for {{Monitoring}} and {{Improving ML Models}}, {arXiv:2003.01668 [cs]}, v(), (2020).">Kang_ModelAssertionsMonitoring_2020</a></sup>

- Want our models to learn causality and not correlation


### Causality {#causality}

Check statistical correlations: covariance, probability theory (joint pdf)

Reichenbach's _Common Cause Principle_

![[2020-07-16_15-10-07_screenshot.png]]

Causality is represented by a graph.

Can we, based on a logical causal reason, recover or fill NaNs with what is
possible? If cold and heating stage 2 is ON, shouldn't heating stage 1 also be
ON?


### Missing Data {#missing-data}

MCAR
: missing completely at random

MAR
: missing at random

MNAR
: missing not at random


### Advantage {#advantage}

We can start doing some interferences onto the models: fix certain parameters to
a value and look at what happens.


## Training-Free Uncertainty Estimation for Neural Networks {#training-free-uncertainty-estimation-for-neural-networks}

<sup id="bafc77216d3d3e99319d77a991c52fa1"><a href="#Mi_TrainingFreeUncertaintyEstimation_2019" title="Mi, Wang, Tian \&amp; Shavit, Training-{{Free Uncertainty Estimation}} for {{Neural Networks}}, {arXiv:1910.04858 [cs]}, v(), (2019).">Mi_TrainingFreeUncertaintyEstimation_2019</a></sup>

No need to retrain the model, reuse the model but add a bit of noise in the
input and the weights.

- Black-box uncertainty estimation: infer-transformation: apply rotations and
    flips to the input
- Gray-box uncertainty estimation: infer-noise and infer-dropout: add noise

Optimal control -> validation set with high uncertainty can be used to fine-tune
a model

Why does this work?


## Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting {#temporal-fusion-transformers-for-interpretable-multi-horizon-time-series-forecasting}

<sup id="1dc369642340f0209273395e79981819"><a href="#Lim_TemporalFusionTransformers_2019" title="">Lim_TemporalFusionTransformers_2019</a></sup>

reference: [[Lim_TemporalFusionTransformers_2019]]

Model takes in:

- Static metadata
- Past inputs
- Future known inputs

Has a multi-head attention layer which is interpretable

[//begin]: # "Autogenerated link references for markdown compatibility"
[[2020-07-16_15-10-07_screenshot.png]: ../attachments/Causality_and_Missing_Data/2020-07-16_15-10-07_screenshot.png "2020-07-16_15-10-07_screenshot.png"
[Lim_TemporalFusionTransformers_2019]: ../articles/Lim_TemporalFusionTransformers_2019.md "Lim_TemporalFusionTransformers_2019: Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"
[//end]: # "Autogenerated link references"