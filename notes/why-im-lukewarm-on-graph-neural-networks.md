---
author:
- Matt Ranger
categories: website
draft: false
lastmod: 2021-01-09 09:35:18-05:00
slug: graphical_models
tags:
- research
- model-graphical
title: Why I’m lukewarm on graph neural networks
---


## Why I’m lukewarm on graph neural networks {#why-i-m-lukewarm-on-graph-neural-networks}

<https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/>

TL;DR: GNNs can provide wins over simpler embedding methods, but we’re at a
point where other research directions matter more

Graphs are basically a way of representing an adjacency matrix:

![[2021-01-05_08-18-59_AdjacencyMatrices_1002.gif.gif]]

If we step out of the pesky realm of physics for a minute, and assume carrying
the full adjacency matrix around isn’t a problem, we solve a bunch of problems.

First, network node embeddings aren’t a thing anymore. A node is a just row in
the matrix, so it’s already a vector of numbers.

Second, all network prediction problems are solved. A powerful enough and
well-tuned model will simply extract all information between the network and
whichever target variable we’re attaching to nodes.

Language models are matrix compression.

Let’s call a first-order embedding of a graph a method that works by directly
factoring the graph’s adjacency matrix or Laplacian matrix. If you embed a graph
using Laplacian Eigenmaps or by taking the principal components of the
Laplacian, that’s first order. Similarly, GloVe is a first-order method on the
graph of word co-occurences. One of my favorites first order methods for graphs
is ProNE, which works as well as most methods while being two orders of
magnitude faster.

A higher-order method embeds the original matrix plus connections of
neighbours-of-neighbours (2nd degree) and deeper k-step connections. GraRep,
shows you can always generate higher-order representations from first order
methods by augmenting the graph matrix.

Most GNN papers in the last 5 years present empirical numbers that are useless
for practitioners to decide on what to use.

Similarly, I find that for many graphs simple first-order methods perform just
as well on graph clustering and node label prediction tasks than higher-order
embedding methods. In fact higher-order methods are massively computationally
wasteful for these usecases.

The problem here is that we don’t know when any method is better than another
and we definitely don’t know the reason.

We’ve known how to train neural networks for well over 40 years. Yet they only
exploded in popularity with AlexNet in 2012. This is because implementations and
hardware came to a point where deep learning was practical.

Efficiency is paramount to progress

Academic code is terrible
<sup id="3ce1f57496e8435ab71c4252828ab366"><a href="#Lipton_TroublingTrendsMachine_2018" title="Lipton \&amp; Steinhardt, Troubling {{Trends}} in {{Machine Learning Scholarship}}, {arXiv:1807.03341 [cs, stat]}, v(), (2018).">Lipton_TroublingTrendsMachine_2018</a></sup>

This is really just Sutton’ Bitter Lesson in action:

General methods that leverage computation are ultimately the most effective, and
by a large margin: <sup id="ce95730c0fc3bcb399ac531b59a904a5"><a href="#Sutton_BitterLesson_2019" title="@misc{Sutton_BitterLesson_2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Rich},
  year = {2019},
  month = mar,
  howpublished = {\url{http://incompleteideas.net/IncIdeas/BitterLesson.html}},
  journal = {The Bitter Lesson},
  keywords = {commentary}
}">Sutton_BitterLesson_2019</a></sup>

Researchers seem to be putting so much effort into architecture, but it doesn’t
matter much in the end because you can approximate anything by stacking more
layers.

Methods that work on the entire graph at once can’t leverage computation,
because they run out of RAM at a certain scale.

Instead of graph models, Poincare embedding seems promising:
<sup id="24b635a239e95702b8cc49e32550d69f"><a href="#Nickel_PoincareEmbeddingsLearning_2017" title="Nickel \&amp; Kiela, Poincar\'e {{Embeddings}} for {{Learning Hierarchical Representations}}, {arXiv:1705.08039 [cs, stat]}, v(), (2017).">Nickel_PoincareEmbeddingsLearning_2017</a></sup>

[//begin]: # "Autogenerated link references for markdown compatibility"
[[2021-01-05_08-18-59_AdjacencyMatrices_1002.gif.gif]: ../attachments/Why_Im_lukewarm_on_graph_neural_networks/2021-01-05_08-18-59_AdjacencyMatrices_1002.gif.gif "2021-01-05_08-18-59_AdjacencyMatrices_1002.gif.gif"
[//end]: # "Autogenerated link references"